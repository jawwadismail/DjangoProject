{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw=pd.read_csv(\"sih/data.csv\")\n",
    "raw['Tweet']=raw.Tweet.apply(lambda x:x.replace('.',''))\n",
    "raw.head()\n",
    "\n",
    "raw.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string \n",
    "import re\n",
    "\n",
    "#Create lists for tweets and label\n",
    "Tweet = []\n",
    "Labels = []\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "wordnet_lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "for row in raw[\"Tweet\"]:\n",
    "    #tokenize words\n",
    "    words = wpt.tokenize(row)\n",
    "    #remove punctuations\n",
    "    clean_words = [word.lower() for word in words if word not in set(string.punctuation)]\n",
    "    #remove stop words\n",
    "    english_stops = nltk.corpus.stopwords.words('english')\n",
    "    characters_to_remove = [\"''\",'``',\"rt\",\"https\",\"’\",\"“\",\"”\",\"\\u200b\",\"--\",\"n't\",\"'s\",\"...\",\"//t.c\" ]\n",
    "    clean_words = [word for word in clean_words if word not in english_stops]\n",
    "    clean_words = [word for word in clean_words if word not in set(characters_to_remove)]\n",
    "    #Lematise words\n",
    "    lemma_list = [wordnet_lemmatizer.lemmatize(word) for word in clean_words]\n",
    "    #print(lemma_list)\n",
    "    Tweet.append(lemma_list)\n",
    "\n",
    "    \n",
    "    \n",
    "for label in raw[\"Text Label\"]:\n",
    "    Labels.append(label)\n",
    "    \n",
    "c=[]\n",
    "    \n",
    "for x in Tweet:\n",
    "    s=''\n",
    "    for y in x:\n",
    "            s=s+' '+y\n",
    "    s = re.sub('[^A-Za-z0-9\" \"]+', '', s)\n",
    "    s=s.lstrip()\n",
    "    c.append(s)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "# have given min_document_freqn = 0.0 ->  which means ignore \n",
    "# terms that appear in less than 1% of the documents \n",
    "\n",
    "# and max_document_freqn = 1.0 ->  which means ignore terms that appear \n",
    "# in more than 100% of the documents\".\n",
    "# In short nothing is to be ignored. all data values would be considered \n",
    "\n",
    "cv_matrix = cv.fit_transform(c)\n",
    "\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# the final preprocessing step is to divide data into training and test sets\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(cv_matrix, Labels, test_size = 0.20,random_state=0)\n",
    "\n",
    "# TYPE your Code here\n",
    "# Training the Algorithm. Here we would use simple SVM , i.e linear SVM\n",
    "#simple SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svclassifier=SVC(kernel='linear')\n",
    "\n",
    "\n",
    "\n",
    "# classifying linear data\n",
    "\n",
    "\n",
    "# kernel can take many values like\n",
    "# Gaussian, polynomial, sigmoid, or computable kernel\n",
    "# fit the model over data\n",
    "\n",
    "svclassifier.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# Making Predictions\n",
    "\n",
    "y_pred=svclassifier.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluating the Algorithm\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "\n",
    "# Remember : for evaluating classification-based ML algo use  \n",
    "# confusion_matrix, classification_report and accuracy_score.\n",
    "# And for evaluating regression-based ML Algo use Mean Squared Error(MSE), ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n",
    "tv_matrix = tv.fit_transform(c)\n",
    "tv_matrix = tv_matrix.toarray()\n",
    "\n",
    "vocab = tv.get_feature_names()\n",
    "\n",
    "lol=pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# the final preprocessing step is to divide data into training and test sets\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(tv_matrix, Labels, test_size = 0.20,random_state=0)\n",
    "\n",
    "# TYPE your Code here\n",
    "# Training the Algorithm. Here we would use simple SVM , i.e linear SVM\n",
    "#simple SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svclassifier=SVC(kernel='linear')\n",
    "\n",
    "\n",
    "\n",
    "# classifying linear data\n",
    "\n",
    "\n",
    "# kernel can take many values like\n",
    "# Gaussian, polynomial, sigmoid, or computable kernel\n",
    "# fit the model over data\n",
    "\n",
    "svclassifier.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# Making Predictions\n",
    "\n",
    "y_pred=svclassifier.predict(X_test)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "\n",
    "# Evaluating the Algorithm\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "\n",
    "# Remember : for evaluating classification-based ML algo use  \n",
    "# confusion_matrix, classification_report and accuracy_score.\n",
    "# And for evaluating regression-based ML Algo use Mean Squared Error(MSE), ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "model = GaussianNB()\n",
    "\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "predicted=model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict_proba(X_test)[1]\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(probability=True)\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear', probability=True)\n",
    "svm.fit(X_train, y_train)\n",
    "LABEL=svm.predict(X_test)\n",
    "print(LABEL[1:10])\n",
    "output_proba = svm.predict_proba(X_test)\n",
    "print(output_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tweepy\n",
    "\n",
    "\n",
    "# credentials  --> put your credentials here\n",
    "consumer_key = \"EgjCgVxayfmnj68PXiVKu1PVj\"\n",
    "consumer_secret = \"QFszM57LLN4GyIOmPBG9Q1bjWceDj7yGBIEuF2xWCyt93BbZ8Z\"\n",
    "access_token = \"1199917326285426690-MWVeRP6oBVW7b6X5FEHJhwpNBB2bXf\"\n",
    "access_token_secret = \"BqqWCmW1hUGlZhLLyXyjeFYVByDoJlPGIYwpGU949Y89x\"\n",
    "\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "L  = []\n",
    "author=[]\n",
    "\n",
    "\n",
    "class CustomStreamListener(tweepy.StreamListener):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.counter = 0\n",
    "        self.limit = 10\n",
    "        \n",
    "    def on_status(self, status):\n",
    "            #print(status.text)\n",
    "            global L\n",
    "            L.append(status.text) \n",
    "            author.append(status.user.screen_name)\n",
    "            self.counter += 1\n",
    "            if self.counter < self.limit:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        print >> sys.stderr, 'Encountered error with status code:', status_code\n",
    "        return True # Don't kill the stream\n",
    "\n",
    "    def on_timeout(self):\n",
    "        print >> sys.stderr, 'Timeout...'\n",
    "        return False # Don't kill the stream\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "sapi = tweepy.streaming.Stream(auth, CustomStreamListener())    \n",
    "sapi.filter(locations=[-74.36141,40.55905,-73.704977,41.01758])\n",
    "main = pd.DataFrame(L,columns=['Tweet'])\n",
    "\n",
    "\n",
    "print(main)\n",
    "print(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts=[]\n",
    "for row in main[\"Tweet\"]:\n",
    "    #tokenize words\n",
    "    words = wpt.tokenize(row)\n",
    "    #remove punctuations\n",
    "    clean_words = [word.lower() for word in words if word not in set(string.punctuation)]\n",
    "    #remove stop words\n",
    "    english_stops = nltk.corpus.stopwords.words('english')\n",
    "    characters_to_remove = [\"''\",'``',\"rt\",\"https\",\"’\",\"“\",\"”\",\"\\u200b\",\"--\",\"n't\",\"'s\",\"...\",\"//t.c\" ]\n",
    "    clean_words = [word for word in clean_words if word not in english_stops]\n",
    "    clean_words = [word for word in clean_words if word not in set(characters_to_remove)]\n",
    "    #Lematise words\n",
    "    lemma_list = [wordnet_lemmatizer.lemmatize(word) for word in clean_words]\n",
    "    #print(lemma_list)\n",
    "    posts.append(lemma_list)\n",
    "    \n",
    "    \n",
    "refine=[]\n",
    "    \n",
    "for x in posts:\n",
    "    s=''\n",
    "    for y in x:\n",
    "            s=s+' '+y\n",
    "    s = re.sub('[^A-Za-z0-9\" \"]+', '', s)\n",
    "    s=s.lstrip()\n",
    "    refine.append(s)    \n",
    "\n",
    "    \n",
    "print(refine)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#re_tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n",
    "re_tv_matrix = tv.transform(refine)\n",
    "re_tv_matrix = re_tv_matrix.toarray()\n",
    "\n",
    "print(re_tv_matrix)\n",
    "\n",
    "OUTPUT=svm.predict(re_tv_matrix)\n",
    "\n",
    "\n",
    "SEVERITY=svm.predict_proba(re_tv_matrix)\n",
    "\n",
    "\n",
    "print(OUTPUT)\n",
    "print(SEVERITY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=list()\n",
    "for i in main['Tweet']:\n",
    "    index = list(main['Tweet']).index(i)\n",
    "    if(OUTPUT[index]==\"Bullying\"):\n",
    "        ind.append(author[index])\n",
    "        \n",
    "        \n",
    "print(ind)        \n",
    "\n",
    "limit=0\n",
    "\n",
    "timeline=[]\n",
    "\n",
    "CRIM_SCORE=[]\n",
    "CRIM_NAME=[]\n",
    "\n",
    "for i in ind:\n",
    "    #print(i)\n",
    "    #print(limit)\n",
    "    for po in tweepy.Cursor(api.user_timeline, screen_name=i, tweet_mode=\"extended\",exclude='retweets').items():\n",
    "        if(limit<10):\n",
    "            if (not po.retweeted) and ('RT @' not in po.full_text):\n",
    "                timeline.append(po.full_text)\n",
    "                #print(po.full_text)\n",
    "                limit=limit+1\n",
    "        else:\n",
    "            limit=0\n",
    "            break\n",
    "        \n",
    "    final= pd.DataFrame(timeline,columns=['Tweet'])\n",
    "    posts_culprit=[]\n",
    "    for row in final[\"Tweet\"]:\n",
    "        #tokenize words\n",
    "        words = wpt.tokenize(row)\n",
    "        #remove punctuations\n",
    "        clean_words = [word.lower() for word in words if word not in set(string.punctuation)]\n",
    "        #remove stop words\n",
    "        english_stops = nltk.corpus.stopwords.words('english')\n",
    "        characters_to_remove = [\"''\",'``',\"rt\",\"https\",\"’\",\"“\",\"”\",\"\\u200b\",\"--\",\"n't\",\"'s\",\"...\",\"//t.c\" ]\n",
    "        clean_words = [word for word in clean_words if word not in english_stops]\n",
    "        clean_words = [word for word in clean_words if word not in set(characters_to_remove)]\n",
    "        #Lematise words\n",
    "        lemma_list = [wordnet_lemmatizer.lemmatize(word) for word in clean_words]\n",
    "        #print(lemma_list)\n",
    "        posts_culprit.append(lemma_list)\n",
    "\n",
    "\n",
    "    refine_culprit=[]\n",
    "\n",
    "    for x in posts:\n",
    "        s=''\n",
    "        for y in x:\n",
    "                s=s+' '+y\n",
    "        s = re.sub('[^A-Za-z0-9\" \"]+', '', s)\n",
    "        s=s.lstrip()\n",
    "        refine_culprit.append(s)    \n",
    "\n",
    "\n",
    "    #print(refine_culprit)    \n",
    "\n",
    "    final_tv_matrix = tv.transform(refine)\n",
    "    final_tv_matrix = final_tv_matrix.toarray()\n",
    "\n",
    "    #print(re_tv_matrix)\n",
    "\n",
    "    OUTPUT_FINAL=svm.predict(final_tv_matrix)\n",
    "\n",
    "\n",
    "    SEVERITY_FINAL=svm.predict_proba(final_tv_matrix)\n",
    "\n",
    "\n",
    "    \n",
    "    #print(OUTPUT_FINAL)\n",
    "    #print(SEVERITY_FINAL)\n",
    "    \n",
    "    for j in range(SEVERITY.shape[0]):\n",
    "        x\n",
    "        if(OUTPUT[j]==\"Bullying\"):    \n",
    "            x=SEVERITY[j][0]\n",
    "            #print(x,i)\n",
    "            for k in range(SEVERITY_FINAL.shape[0]):\n",
    "                if SEVERITY_FINAL[k][0]>0.5:\n",
    "                    #print(\"ADD\",SEVERITY_FINAL[k][0])\n",
    "                    x=x+SEVERITY_FINAL[k][0]\n",
    "                    #print(\"ADD\")\n",
    "                else:\n",
    "                    #print(\"SUB\",SEVERITY_FINAL[k][0])\n",
    "                    x=x-SEVERITY_FINAL[k][1]\n",
    "                    #print(\"SUB\")\n",
    "                #print(x)    \n",
    "                break\n",
    "            CRIM_SCORE.append(x)\n",
    "            CRIM_NAME.append(i)    \n",
    "    \n",
    "            \n",
    "    F_SCORE=[]\n",
    "    F_NAME=[]\n",
    "    for m in CRIM_SCORE: \n",
    "        if m not in F_SCORE: \n",
    "            F_SCORE.append(m) \n",
    "        \n",
    "    for m in CRIM_NAME: \n",
    "        if m not in F_NAME: \n",
    "            F_NAME.append(m) \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "print((F_SCORE)) \n",
    "print(len(F_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_culprit=[]\n",
    "for row in final[\"Tweet\"]:\n",
    "    #tokenize words\n",
    "    words = wpt.tokenize(row)\n",
    "    #remove punctuations\n",
    "    clean_words = [word.lower() for word in words if word not in set(string.punctuation)]\n",
    "    #remove stop words\n",
    "    english_stops = nltk.corpus.stopwords.words('english')\n",
    "    characters_to_remove = [\"''\",'``',\"rt\",\"https\",\"’\",\"“\",\"”\",\"\\u200b\",\"--\",\"n't\",\"'s\",\"...\",\"//t.c\" ]\n",
    "    clean_words = [word for word in clean_words if word not in english_stops]\n",
    "    clean_words = [word for word in clean_words if word not in set(characters_to_remove)]\n",
    "    #Lematise words\n",
    "    lemma_list = [wordnet_lemmatizer.lemmatize(word) for word in clean_words]\n",
    "    #print(lemma_list)\n",
    "    posts_culprit.append(lemma_list)\n",
    "    \n",
    "    \n",
    "refine_culprit=[]\n",
    "    \n",
    "for x in posts:\n",
    "    s=''\n",
    "    for y in x:\n",
    "            s=s+' '+y\n",
    "    s = re.sub('[^A-Za-z0-9\" \"]+', '', s)\n",
    "    s=s.lstrip()\n",
    "    refine_culprit.append(s)    \n",
    "\n",
    "    \n",
    "print(refine_culprit)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tv_matrix = tv.transform(refine)\n",
    "final_tv_matrix = final_tv_matrix.toarray()\n",
    "\n",
    "print(re_tv_matrix)\n",
    "\n",
    "OUTPUT_FINAL=svm.predict(re_tv_matrix)\n",
    "\n",
    "\n",
    "SEVERITY_FINAL=svm.predict_proba(re_tv_matrix)\n",
    "\n",
    "\n",
    "print(OUTPUT_FINAL)\n",
    "print(SEVERITY_FINAL)\n",
    "\n",
    "for i in range(SEVERITY.shape[0]):\n",
    "    print(SEVERITY[i][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "24761827-1012-441c-8b45-b4ea925a3d5c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}